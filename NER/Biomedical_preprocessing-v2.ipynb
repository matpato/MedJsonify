{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required NLTK resources\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wEVnpCFcgQz",
        "outputId": "f5d55ea2-30bd-45db-f1e0-1a99925efb24",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Common biomedical abbreviations and their expanded forms\n",
        "bio_abbreviations = {\n",
        "    \"AD\": \"Alzheimer's disease\",\n",
        "    \"MI\": \"myocardial infarction\",\n",
        "    \"HTN\": \"hypertension\",\n",
        "    \"DM\": \"diabetes mellitus\",\n",
        "    \"CHF\": \"congestive heart failure\",\n",
        "    \"COPD\": \"chronic obstructive pulmonary disease\",\n",
        "    \"RA\": \"rheumatoid arthritis\",\n",
        "    \"MS\": \"multiple sclerosis\",\n",
        "    \"ASMD\": \"ASM-deficient Niemann-Pick disease\",\n",
        "    # Add more as needed\n",
        "}\n",
        "\n",
        "# Common misspellings of drug names\n",
        "drug_spelling_corrections = {\n",
        "    \"acetaminophen\": [\"acetaminophen\", \"acetaminophine\", \"acetaminofin\"],\n",
        "    \"ibuprofen\": [\"ibuprofen\", \"ibuprofin\", \"ibuprophen\"],\n",
        "    \"amoxicillin\": [\"amoxicillin\", \"amoxicilin\", \"amoxicillan\"],\n",
        "    # Add more as needed\n",
        "}\n",
        "\n",
        "# Create reverse mapping for drug spelling corrections\n",
        "drug_spelling_map = {}\n",
        "for correct, variants in drug_spelling_corrections.items():\n",
        "    for variant in variants:\n",
        "        if variant != correct:\n",
        "            drug_spelling_map[variant] = correct"
      ],
      "metadata": {
        "id": "yUYHbsBScqND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_encoding_issues(text):\n",
        "    \"\"\"\n",
        "    Fix common encoding issues in text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text with potential encoding issues\n",
        "\n",
        "    Returns:\n",
        "        str: Text with fixed encoding issues\n",
        "    \"\"\"\n",
        "    # Replace common problematic characters\n",
        "    replacements = {\n",
        "        '\\x92': \"'\",    # Right single quotation mark\n",
        "        '\\x93': '\"',    # Left double quotation mark\n",
        "        '\\x94': '\"',    # Right double quotation mark\n",
        "        '\\x96': '-',    # En dash\n",
        "        '\\x97': '-',    # Em dash\n",
        "        '\\xa0': ' ',    # Non-breaking space\n",
        "        '&amp;': '&',   # HTML ampersand\n",
        "        '&lt;': '<',    # HTML less than\n",
        "        '&gt;': '>',    # HTML greater than\n",
        "    }\n",
        "\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "dBNpst6Ycybp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_punctuation(text, keep_punctuation=True):\n",
        "    \"\"\"\n",
        "    Standardize punctuation in text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text\n",
        "        keep_punctuation (bool): Whether to keep biomedically relevant punctuation\n",
        "\n",
        "    Returns:\n",
        "        str: Text with standardized punctuation\n",
        "    \"\"\"\n",
        "    if keep_punctuation:\n",
        "        # Replace multiple dashes with single dash (but keep the dash)\n",
        "        text = re.sub(r'-+', '-', text)\n",
        "\n",
        "        # Ensure spaces around punctuation except for specific cases\n",
        "        # Keep punctuation in patterns like \"COVID-19\", \"5-HTP\", \"50mg\"\n",
        "        for punct in [',', '.', ';', ':', '!', '?']:\n",
        "            text = re.sub(f'(?<![A-Za-z0-9]){re.escape(punct)}', f' {punct} ', text)\n",
        "\n",
        "        # Standardize parentheses with spaces\n",
        "        text = re.sub(r'\\(', ' ( ', text)\n",
        "        text = re.sub(r'\\)', ' ) ', text)\n",
        "\n",
        "        # Fix spaces\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "    else:\n",
        "        # Remove punctuation entirely (not recommended for biomedical NER)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "TADyOeO5c3Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_case(text, preserve_case=False):\n",
        "    \"\"\"\n",
        "    Normalize the case of text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text\n",
        "        preserve_case (bool): Whether to preserve the original case\n",
        "\n",
        "    Returns:\n",
        "        str: Text with normalized case\n",
        "    \"\"\"\n",
        "    if not preserve_case:\n",
        "        text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "7kfHp1kRke3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_abbreviations(text, abbreviations=bio_abbreviations):\n",
        "    \"\"\"\n",
        "    Expand common biomedical abbreviations.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text\n",
        "        abbreviations (dict): Dictionary of abbreviations and their expanded forms\n",
        "\n",
        "    Returns:\n",
        "        str: Text with expanded abbreviations\n",
        "    \"\"\"\n",
        "    # Create case-insensitive dictionary (all keys to lowercase)\n",
        "    abbrev_lower = {k.lower(): v for k, v in abbreviations.items()}\n",
        "\n",
        "    # Tokenize with word boundaries to handle punctuation\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "    for word in words:\n",
        "        lower_word = word.lower()\n",
        "\n",
        "        if lower_word in abbrev_lower:\n",
        "            # Replace the abbreviation with its expanded form\n",
        "            pattern = r'\\b' + re.escape(word) + r'\\b'\n",
        "            text = re.sub(pattern, abbrev_lower[lower_word], text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "vYGODj1akhVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_drug_spelling(text, spelling_map=drug_spelling_map):\n",
        "    \"\"\"\n",
        "    Correct common misspellings of drug names.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text\n",
        "        spelling_map (dict): Dictionary mapping misspelled drugs to their correct spelling\n",
        "\n",
        "    Returns:\n",
        "        str: Text with corrected drug spellings\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    for i, word in enumerate(words):\n",
        "        lower_word = word.lower()\n",
        "        if lower_word in spelling_map:\n",
        "            # Replace with correct spelling but preserve case pattern\n",
        "            if word.isupper():\n",
        "                words[i] = spelling_map[lower_word].upper()\n",
        "            elif word[0].isupper():\n",
        "                words[i] = spelling_map[lower_word].capitalize()\n",
        "            else:\n",
        "                words[i] = spelling_map[lower_word]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "aACvyUbYkj0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, standard_stopwords, custom_stopwords=None):\n",
        "    \"\"\"\n",
        "    Remove stopwords from text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text\n",
        "        standard_stopwords (set): Set of standard stopwords\n",
        "        custom_stopwords (list, optional): List of custom stopwords\n",
        "\n",
        "    Returns:\n",
        "        str: Text with stopwords removed\n",
        "    \"\"\"\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Filter out standard stopwords\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in standard_stopwords]\n",
        "\n",
        "    # Filter out custom stopwords if provided\n",
        "    if custom_stopwords:\n",
        "        custom_stopwords_lower = [word.lower() for word in custom_stopwords]\n",
        "        filtered_tokens = [token for token in filtered_tokens\n",
        "                          if token.lower() not in custom_stopwords_lower]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "dcmQsQtGkmY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, preserve_case=False, keep_punctuation=True,\n",
        "                   remove_stops=True, custom_stopwords=None):\n",
        "    \"\"\"\n",
        "    Apply all preprocessing steps to the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text\n",
        "        preserve_case (bool): Whether to preserve the original case\n",
        "        keep_punctuation (bool): Whether to keep biomedically relevant punctuation\n",
        "        remove_stops (bool): Whether to remove stopwords\n",
        "        custom_stopwords (list): Custom stopwords to remove\n",
        "\n",
        "    Returns:\n",
        "        str: Fully preprocessed text\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Apply preprocessing steps in sequence\n",
        "    text = fix_encoding_issues(text)\n",
        "    text = standardize_punctuation(text, keep_punctuation)\n",
        "\n",
        "    text = expand_abbreviations(text)\n",
        "    text = correct_drug_spelling(text)\n",
        "\n",
        "    if not preserve_case:\n",
        "        text = normalize_case(text)\n",
        "\n",
        "    # Remove stopwords if specified\n",
        "    if remove_stops:\n",
        "        standard_stopwords = set(stopwords.words('english'))\n",
        "        text = remove_stopwords(text, standard_stopwords, custom_stopwords)\n",
        "\n",
        "    # Ensure clean whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "QijfOVOLkx02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_json_file(input_file_path, output_file_path, fields_to_process=None,\n",
        "                     preserve_case=False, keep_punctuation=True, remove_stops=True):\n",
        "    \"\"\"\n",
        "    Process text fields in a JSON file.\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): Path to the input JSON file\n",
        "        output_file_path (str): Path to save the processed JSON file\n",
        "        fields_to_process (list): List of specific fields to process\n",
        "        preserve_case (bool): Whether to preserve the original case\n",
        "        keep_punctuation (bool): Whether to keep biomedically relevant punctuation\n",
        "        remove_stops (bool): Whether to remove stopwords\n",
        "    \"\"\"\n",
        "    # Define custom stopwords for biomedical documents\n",
        "    custom_stopwords = [\n",
        "        \"Warnings\", \"Precautions\", \"Use\", \"Specific\", \"Populations\",\n",
        "        \"see\", \"contraindications\", \"indications\", \"dosage\", \"administration\",\n",
        "        \"adverse\", \"reactions\", \"drug\", \"interactions\", \"clinical\", \"studies\"\n",
        "    ]\n",
        "\n",
        "    # Load the JSON file\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # If specific fields are provided, only process those\n",
        "    if fields_to_process:\n",
        "        for field in fields_to_process:\n",
        "            if field in data and isinstance(data[field], str):\n",
        "                # Store original field value\n",
        "                data[f\"{field}_original\"] = data[field]\n",
        "\n",
        "                # Apply text preprocessing\n",
        "                data[field] = preprocess_text(\n",
        "                    data[field],\n",
        "                    preserve_case=preserve_case,\n",
        "                    keep_punctuation=keep_punctuation,\n",
        "                    remove_stops=remove_stops,\n",
        "                    custom_stopwords=custom_stopwords\n",
        "                )\n",
        "    else:\n",
        "        # Process all string fields in the JSON\n",
        "        processed_data = process_json_object(\n",
        "            data,\n",
        "            preserve_case=preserve_case,\n",
        "            keep_punctuation=keep_punctuation,\n",
        "            remove_stops=remove_stops,\n",
        "            custom_stopwords=custom_stopwords\n",
        "        )\n",
        "        data = processed_data\n",
        "\n",
        "    # Save the processed JSON\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Processed JSON saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "3s1747pVk09i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_json_object(obj, preserve_case=False, keep_punctuation=True,\n",
        "                       remove_stops=True, custom_stopwords=None):\n",
        "    \"\"\"\n",
        "    Recursively process a JSON object, preprocessing text fields.\n",
        "\n",
        "    Args:\n",
        "        obj: JSON object (dict, list, or primitive value)\n",
        "        preserve_case (bool): Whether to preserve the original case\n",
        "        keep_punctuation (bool): Whether to keep biomedically relevant punctuation\n",
        "        remove_stops (bool): Whether to remove stopwords\n",
        "        custom_stopwords (list): Custom stopwords to remove\n",
        "\n",
        "    Returns:\n",
        "        The processed JSON object\n",
        "    \"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        result = {}\n",
        "        for key, value in obj.items():\n",
        "            if isinstance(value, str):\n",
        "                # Preprocess text fields\n",
        "                result[f\"{key}_original\"] = value\n",
        "                result[key] = preprocess_text(\n",
        "                    value,\n",
        "                    preserve_case=preserve_case,\n",
        "                    keep_punctuation=keep_punctuation,\n",
        "                    remove_stops=remove_stops,\n",
        "                    custom_stopwords=custom_stopwords\n",
        "                )\n",
        "            else:\n",
        "                # Recursively process non-string fields\n",
        "                result[key] = process_json_object(\n",
        "                    value,\n",
        "                    preserve_case=preserve_case,\n",
        "                    keep_punctuation=keep_punctuation,\n",
        "                    remove_stops=remove_stops,\n",
        "                    custom_stopwords=custom_stopwords\n",
        "                )\n",
        "        return result\n",
        "    elif isinstance(obj, list):\n",
        "        return [process_json_object(\n",
        "            item,\n",
        "            preserve_case=preserve_case,\n",
        "            keep_punctuation=keep_punctuation,\n",
        "            remove_stops=remove_stops,\n",
        "            custom_stopwords=custom_stopwords\n",
        "        ) for item in obj]\n",
        "    else:\n",
        "        # Return primitive values unchanged\n",
        "        return obj"
      ],
      "metadata": {
        "id": "xl_tlUK4k6lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"sample_data/0bdf77ae-3639-49c1-b7c7-533f9d073084.json\"  # Replace with your input file\n",
        "output_file = \"sample_data/0bdf77ae-3639-49c1-b7c7-533f9d073084_clean.json\" # Custom stopwords for specific fields\n",
        "\n",
        "# Process only specific fields\n",
        "fields_to_process = [\"contraindications\", \"indications\", \"warningsAndPrecautions\", \"adverseReactions\"]\n",
        "\n",
        "# Process the JSON file\n",
        "process_json_file(\n",
        "    input_file,\n",
        "    output_file,\n",
        "    fields_to_process=fields_to_process,\n",
        "    preserve_case=False,\n",
        "    keep_punctuation=True,\n",
        "    remove_stops=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W32FODsOlC79",
        "outputId": "1d9b6ed0-2e8a-4611-c472-217b7cbe4506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed JSON saved to sample_data/0bdf77ae-3639-49c1-b7c7-533f9d073084_clean.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from owlready2 import *\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQddZt8KLLXO",
        "outputId": "56f44337-db80-4300-eeb4-25fcdaccde76"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xkH6BLTSVLE",
        "outputId": "288b113b-1ad4-4652-9fc4-e83e18e4256e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ORDO ontology\n",
        "print(\"Loading ORDO ontology...\")\n",
        "onto = get_ontology(\"https://www.orphadata.com/data/ontologies/ordo/last_version/ORDO_en_4.6.owl\").load()\n",
        "print(\"Ontology loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScpX7zxSNCBu",
        "outputId": "5cc4ab81-f289-47c7-f187-3b4ee0d9bcd7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ORDO ontology...\n",
            "Ontology loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of all disease terms in the ontology for faster lookup\n",
        "disease_terms = {}\n",
        "for cls in onto.classes():\n",
        "    if hasattr(cls, \"label\") and cls.label:\n",
        "        for label in cls.label:\n",
        "            disease_terms[label.lower()] = cls.iri\n",
        "\n",
        "    # Add synonyms\n",
        "    for prop in [\"hasExactSynonym\", \"hasRelatedSynonym\", \"hasNarrowSynonym\"]:\n",
        "        if hasattr(cls, prop):\n",
        "            for synonym in getattr(cls, prop):\n",
        "                disease_terms[synonym.lower()] = cls.iri"
      ],
      "metadata": {
        "id": "kqWQpz6ZNVNl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loaded {len(disease_terms)} disease terms from ontology\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRBX29xQL5f0",
        "outputId": "64a209db-e797-4026-8d37-de31b1e65bd9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 15579 disease terms from ontology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_drug_names(text):\n",
        "    \"\"\"Extract drug names from text using rule-based patterns\"\"\"\n",
        "\n",
        "    # Common drug name suffixes by class\n",
        "    drug_patterns = [\n",
        "        r'\\b\\w+(?:mab|ximab|zumab|umab)\\b',  # Monoclonal antibodies\n",
        "        r'\\b\\w+(?:tinib|pib|nib|fib)\\b',  # Kinase inhibitors\n",
        "        r'\\b\\w+(?:olol)\\b',  # Beta blockers\n",
        "        r'\\b\\w+(?:pril|sartan)\\b',  # ACE inhibitors and ARBs\n",
        "        r'\\b\\w+(?:oxacin|cycline|cillin)\\b',  # Antibiotics\n",
        "        r'\\b\\w+(?:zepam|azepam|azolam)\\b',  # Benzodiazepines\n",
        "        r'\\b\\w+(?:statin)\\b',  # Statins\n",
        "        r'\\b\\w+(?:conazole)\\b',  # Antifungals\n",
        "        r'\\b\\w+(?:zosin)\\b',  # Alpha blockers\n",
        "        r'\\b\\w+(?:dipine|pazil)\\b',  # Calcium channel blockers\n",
        "        r'\\b\\w+(?:barb)\\b',  # Barbiturates\n",
        "        r'\\b\\w+(?:navir)\\b',  # HIV protease inhibitors\n",
        "        r'\\b\\w+(?:setron)\\b',  # 5-HT3 antagonists\n",
        "\n",
        "        # Common drug patterns with capitalization\n",
        "        r'\\b[A-Z][a-z]+(?:[A-Z][a-z]+)+\\b',  # CamelCase drug names\n",
        "        r'\\b[A-Z][a-z]+\\b'  # Capitalized words (potential brand names)\n",
        "    ]\n",
        "\n",
        "    # Find all matches\n",
        "    all_matches = []\n",
        "    for pattern in drug_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        all_matches.extend(matches)\n",
        "\n",
        "    # Common words to exclude (non-drug words that might match patterns)\n",
        "    exclude_words = ['section', 'system', 'central', 'nervous', 'treatment',\n",
        "                     'therapy', 'usage', 'patients', 'studies', 'clinical',\n",
        "                     'indication', 'contraindication', 'reaction']\n",
        "\n",
        "    # Filter results\n",
        "    filtered_matches = [m for m in all_matches\n",
        "                       if m.lower() not in exclude_words\n",
        "                       and len(m) > 3]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_matches = []\n",
        "    for match in filtered_matches:\n",
        "        if match not in unique_matches:\n",
        "            unique_matches.append(match)\n",
        "\n",
        "    return unique_matches"
      ],
      "metadata": {
        "id": "WAPN5bIMLJe9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_disease_entities(text):\n",
        "    \"\"\"Extract disease entities from biomedical text\"\"\"\n",
        "\n",
        "    # Pre-process text\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Generic patterns for disease entities that handle hyphenated terms\n",
        "    disease_patterns = [\n",
        "        r'(?:invasive|severe)\\s+[\\w-]+\\s+infections?',\n",
        "        r'[\\w-]+\\s+(?:disease|disorder|syndrome|deficiency)',\n",
        "        r'(?:acute|chronic)\\s+[\\w-]+\\s+[\\w-]+',\n",
        "        r'[\\w-]+(?:-versus-[\\w-]+)?\\s+disease',\n",
        "        r'[\\w-]+\\s+malignancies',\n",
        "        r'invasive\\s+(?:[\\w-]+)\\s+infections?',\n",
        "        r'[\\w-]+\\s+leukemia',\n",
        "        r'[\\w-]+\\s+lymphoma',\n",
        "        r'[\\w-]+\\s+lysis\\s+syndrome',\n",
        "        r'(?:hyper|hypo)[\\w-]+emia'\n",
        "    ]\n",
        "\n",
        "    # Find disease mentions\n",
        "    disease_mentions = []\n",
        "\n",
        "    # Search using general patterns\n",
        "    for pattern in disease_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        disease_mentions.extend(matches)\n",
        "\n",
        "    # Use NLP approach for more complex extractions\n",
        "    for sentence in sentences:\n",
        "        # First, preserve hyphenated terms by temporarily replacing hyphens with a special marker\n",
        "        preserved_sentence = re.sub(r'(\\w+)-(\\w+)', r'\\1_HYPHEN_\\2', sentence)\n",
        "\n",
        "        # Now tokenize with standard tokenizer\n",
        "        tokens = word_tokenize(preserved_sentence)\n",
        "\n",
        "        # Restore hyphens\n",
        "        tokens = [token.replace('_HYPHEN_', '-') for token in tokens]\n",
        "\n",
        "        # Apply POS tagging\n",
        "        tagged = pos_tag(tokens)\n",
        "\n",
        "        # Build noun phrases\n",
        "        current_np = []\n",
        "        noun_phrases = []\n",
        "\n",
        "        for word, tag in tagged:\n",
        "            if tag.startswith('JJ') or tag.startswith('NN'):\n",
        "                current_np.append(word)\n",
        "            elif current_np:\n",
        "                if len(current_np) > 1:  # Only keep multi-word phrases\n",
        "                    noun_phrases.append(' '.join(current_np))\n",
        "                current_np = []\n",
        "\n",
        "        if current_np and len(current_np) > 1:\n",
        "            noun_phrases.append(' '.join(current_np))\n",
        "\n",
        "        # Filter noun phrases to find disease candidates\n",
        "        disease_indicators = ['disease', 'disorder', 'syndrome', 'infection',\n",
        "                             'deficiency', 'malignancy', 'cancer', 'leukemia',\n",
        "                             'lymphoma', 'transplant', 'neutropenia']\n",
        "\n",
        "        for np in noun_phrases:\n",
        "            if any(indicator in np.lower() for indicator in disease_indicators):\n",
        "                disease_mentions.append(np)\n",
        "\n",
        "    # Remove duplicates and normalize\n",
        "    unique_diseases = []\n",
        "    for disease in disease_mentions:\n",
        "        normalized = disease.lower().strip()\n",
        "        if normalized not in [d.lower() for d in unique_diseases] and len(normalized) > 3:\n",
        "            unique_diseases.append(disease)\n",
        "\n",
        "    return unique_diseases"
      ],
      "metadata": {
        "id": "mHD_MgMaOs5U"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_disease_in_ontology(disease_name, disease_terms_dict):\n",
        "    \"\"\"Find a disease in the ontology using the prebuilt dictionary\"\"\"\n",
        "    search_name = disease_name.lower()\n",
        "\n",
        "    # Try exact match first\n",
        "    if search_name in disease_terms_dict:\n",
        "        return disease_terms_dict[search_name]\n",
        "\n",
        "    # Try substring matching with scoring\n",
        "    matches = []\n",
        "    for term, iri in disease_terms_dict.items():\n",
        "        # Check if the disease name is a substring of the ontology term\n",
        "        if search_name in term:\n",
        "            similarity = len(search_name) / len(term)\n",
        "            matches.append((iri, similarity))\n",
        "        # Check if the ontology term is a substring of the disease name\n",
        "        elif term in search_name:\n",
        "            similarity = len(term) / len(search_name)\n",
        "            matches.append((iri, similarity))\n",
        "\n",
        "    # Sort by similarity score (higher is better)\n",
        "    matches.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if matches and matches[0][1] > 0.5:  # Threshold for decent match\n",
        "        return matches[0][0]\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "cuz6Z8utN-OF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_json_file(file_path):\n",
        "    \"\"\"Process a single JSON file to extract drugs and diseases\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Extract sections of interest\n",
        "    indications = data.get(\"indications\", '')\n",
        "    contraindications = data.get(\"contraindications\", '')\n",
        "\n",
        "    # Extract the main drug name\n",
        "    main_drug = data.get('name', '').strip()\n",
        "\n",
        "    # Extract additional drug names from the text\n",
        "    indications_drugs = extract_drug_names(indications)\n",
        "    contraindications_drugs = extract_drug_names(contraindications)\n",
        "\n",
        "    # Extract disease mentions\n",
        "    indications_diseases = extract_disease_entities(indications)\n",
        "    contraindications_diseases = extract_disease_entities(contraindications)\n",
        "\n",
        "    # Find diseases in ontology\n",
        "    indications_disease_ids = []\n",
        "    for disease in indications_diseases:\n",
        "        disease_id = find_disease_in_ontology(disease, disease_terms)\n",
        "        if disease_id:\n",
        "            indications_disease_ids.append((disease, disease_id))\n",
        "\n",
        "    contraindications_disease_ids = []\n",
        "    for disease in contraindications_diseases:\n",
        "        disease_id = find_disease_in_ontology(disease, disease_terms)\n",
        "        if disease_id:\n",
        "            contraindications_disease_ids.append((disease, disease_id))\n",
        "\n",
        "    return {\n",
        "        'file': os.path.basename(file_path),\n",
        "        'main_drug': main_drug,\n",
        "        'indication_drugs': indications_drugs,\n",
        "        'contraindication_drugs': contraindications_drugs,\n",
        "        'indication_diseases': indications_diseases,\n",
        "        'indication_disease_ids': indications_disease_ids,\n",
        "        'contraindication_diseases': contraindications_diseases,\n",
        "        'contraindication_disease_ids': contraindications_disease_ids\n",
        "    }"
      ],
      "metadata": {
        "id": "wTl65wWZMmTR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_directory(directory_path):\n",
        "    \"\"\"Process all JSON files in a directory\"\"\"\n",
        "    results = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith('.json'):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            result = process_json_file(file_path)\n",
        "            results.append(result)\n",
        "    return results"
      ],
      "metadata": {
        "id": "Se7_FsoeMp-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"sample_data/0cf064d0-cf65-4112-8817-ed864f16233e_clean.json\"\n",
        "result = process_json_file(input_file)"
      ],
      "metadata": {
        "id": "-M0DsyIdSHZy"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Main drug: {result['main_drug']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "133-vPbQSkXL",
        "outputId": "a0532a3a-12fc-4d50-8d6d-0ff0506d4999"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main drug: XENPOZYME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nIndication drugs:\")\n",
        "for drug in result['indication_drugs']:\n",
        "    print(f\"- {drug}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2-hThlVSmr3",
        "outputId": "4cbbf12a-6a00-4c6f-ca0d-485985bce588"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Indication drugs:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nContraindication drugs:\")\n",
        "for drug in result['contraindication_drugs']:\n",
        "    print(f\"- {drug}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HywElkXS1mS",
        "outputId": "182f13ec-c6aa-4f0d-a843-b0d2397929b0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contraindication drugs:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nIndication diseases:\")\n",
        "for disease in result['indication_diseases']:\n",
        "    print(f\"- {disease}\")\n",
        "\n",
        "print(\"\\nIndication diseases with IDs:\")\n",
        "for disease, disease_id in result['indication_disease_ids']:\n",
        "    print(f\"- {disease}: {disease_id}\")\n",
        "\n",
        "print(\"\\nContraindication diseases:\")\n",
        "for disease in result['contraindication_diseases']:\n",
        "    print(f\"- {disease}\")\n",
        "\n",
        "print(\"\\nContraindication diseases with IDs:\")\n",
        "for disease, disease_id in result['contraindication_disease_ids']:\n",
        "    print(f\"- {disease}: {disease_id}\")\n",
        "\n",
        "# To process all files in a directory:\n",
        "# results = process_directory(\"/path/to/your/json/files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Mc-EiPKD2M",
        "outputId": "160692b2-71dc-45af-911b-1c7a6332b153"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Indication diseases:\n",
            "- sphingomyelinase deficiency\n",
            "- niemann-pick disease\n",
            "- asm-deficient niemann-pick disease\n",
            "\n",
            "Indication diseases with IDs:\n",
            "- sphingomyelinase deficiency: http://www.orpha.net/ORDO/Orphanet_618899\n",
            "- niemann-pick disease: http://www.orpha.net/ORDO/Orphanet_646\n",
            "\n",
            "Contraindication diseases:\n",
            "\n",
            "Contraindication diseases with IDs:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file2 = \"sample_data/0cf064d0-cf65-4112-8817-ed864f16233e.json\"\n",
        "result = process_json_file(input_file2)"
      ],
      "metadata": {
        "id": "FVnoro84USqQ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Main drug: {result['main_drug']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u65Pe81VUXVv",
        "outputId": "f9d3410a-95a0-4a5f-db71-c59f6841859f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main drug: XENPOZYME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nIndication diseases:\")\n",
        "for disease in result['indication_diseases']:\n",
        "    print(f\"- {disease}\")\n",
        "\n",
        "print(\"\\nIndication diseases with IDs:\")\n",
        "for disease, disease_id in result['indication_disease_ids']:\n",
        "    print(f\"- {disease}: {disease_id}\")\n",
        "\n",
        "print(\"\\nContraindication diseases:\")\n",
        "for disease in result['contraindication_diseases']:\n",
        "    print(f\"- {disease}\")\n",
        "\n",
        "print(\"\\nContraindication diseases with IDs:\")\n",
        "for disease, disease_id in result['contraindication_disease_ids']:\n",
        "    print(f\"- {disease}: {disease_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX8JFER2Ua5U",
        "outputId": "7fc9204d-810c-4808-c453-c078317f04c6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Indication diseases:\n",
            "- sphingomyelinase deficiency\n",
            "- acid sphingomyelinase deficiency\n",
            "\n",
            "Indication diseases with IDs:\n",
            "- sphingomyelinase deficiency: http://www.orpha.net/ORDO/Orphanet_618899\n",
            "- acid sphingomyelinase deficiency: http://www.orpha.net/ORDO/Orphanet_618899\n",
            "\n",
            "Contraindication diseases:\n",
            "\n",
            "Contraindication diseases with IDs:\n"
          ]
        }
      ]
    }
  ]
}